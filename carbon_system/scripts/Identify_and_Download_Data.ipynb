{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify and Download Datasets\n",
    "\n",
    "### Purpose\n",
    "\n",
    "The purpose of this notebook is to identify and download the datasets with pH and pCO2 data deployed at the Ocean Observatory Initiative's (OOI) Global Irminger Array (60$^{\\circ}$N, 39$^{\\circ}$W). OOI deploys the following sensors for measuring the ocean carbon system: Sunburst Sensors, LLC. SAMI-pH (PHSEN) pH, SAMI-pCO$_{2}$ seawater measurements, and the Pro-Oceanus pCO2 sensor measurements. The datasets identified here are used in the **```Data_Analysis```** notebook. \n",
    "\n",
    "\n",
    "### Datasets\n",
    "While this notebook provides an example of interacting via Machine-to-Machine (M2M) with the OOI API in order to search for datasets, the tables below list the PHSEN, PCO2A, and PCO2W instruments deployed at the Global Irminger Array. Additionally, it identifies and lists associated CTD datasets that are needed for the relevant temperature, salinity, pressure, and density data.\n",
    "\n",
    "#### PCO2A\n",
    "\n",
    "| Instrument | Reference Designator | Mooring | Depth | Associated CTD Reference Designator |\n",
    "| ---------- | -------------------- | ------- | ----- | ----------------------------------- |\n",
    "| PCO2A      | GI01SUMO-SBD12-04-PCO2AA000 | Apex Surface Mooring | Surface | GI01SUMO-SBD12-06-METBKA000 |\n",
    "\n",
    "\n",
    "#### PCO2W\n",
    "\n",
    "| Instrument | Reference Designator | Mooring | Depth | Associated CTD Reference Designator |\n",
    "| ---------- | -------------------- | ------- | ----- | ----------------------------------- |\n",
    "| PCO2W      | GI01SUMO-RID16-05-PCO2WB000 | Apex Surface Mooring | 12 m | GI01SUMO-RID16-03-CTDBPF000 |\n",
    "| PCO2W      | GI01SUMO-RII11-02-PCO2WC051 | Apex Surface Mooring | 40 m | GI01SUMO-RII11-02-CTDMOQ031 |\n",
    "| PCO2W      | GI01SUMO-RII11-02-PCO2WC052 | Apex Surface Mooring | 80 m | GI01SUMO-RII11-02-CTDBPP032 |\n",
    "| PCO2W      | GI01SUMO-RII11-02-PCO2WC053 | Apex Surface Mooring | 130 m | GI01SUMO-RII11-02-CTDBPP033 |\n",
    "\n",
    "#### PHSEN\n",
    "| Instrument | Reference Designator | Mooring | Depth | Associated CTD Reference Designator |\n",
    "| ---------- | -------------------- | ------- | ----- | ----------------------------------- |\n",
    "| PHSEN      | GI01SUMO-RII11-02-PHSENE041 | Apex Surface Mooring | 20 m | GI01SUMO-RII11-02-CTDMOQ011 |\n",
    "| PHSEN      | GI01SUMO-RII11-02-PHSENE042 | Apex Surface Mooring | 100 m | GI01SUMO-RII11-02-CTDMOQ013 |\n",
    "| PHSEN      | GI03FLMA-RIS01-04-PHSENF000 | Flanking Mooring A | 30 m | GI03FLMA-RIM01-02-CTDMOG040 |\n",
    "| PHSEN      | GI01SUMO-RII11-02-CTDMOQ013 | Flanking Mooring B | 30 m | GI01SUMO-RII11-02-CTDMOQ013 |\n",
    "\n",
    "### Methods\n",
    "\n",
    "I use the following procedure for identifying the OOI carbon system and associated CTD datasets from OOI, requesting metadata, downloading the data, and preparing and cleaning the data. This notebook makes use of the **```OOINet```** module hosted at https://github.com/reedan88/OOINet as well as some functions from https://github.com/oceanobservatories/ooi-data-explorations. In order to access and download data from OOI via the API, you MUST register at https://ooinet.oceanobservatories.org and save your **username** and **api token** in a yaml (or similar) file.\n",
    "\n",
    "First, the OOI API is pinged for available datasets as well as their common (or \"English\") names. Then, a specific reference designator (refdes) is selected. Then, I request the metadata for the given refdes. This returns a json file which is reshaped into a dataframe which contains all of the available data stream, data variables, data types, etc for the given reference designator. The metadata is used to filter for the relevant data streams to request from OOI Gold Copy THREDDS server. The Gold Copy THREDDS is utilized because it is a mostly static collection of OOI datasets which are updated only once-a-day, and is thus much faster to request and download. Note, however, that if you need very recent data you will have to use the base THREDDS server and that while most instruments are in the Gold Copy, there are few datasets which have not been migrated.\n",
    "\n",
    "Next, with a selected \"method\" and \"datastream\" to request a dataset, we pass that to OOI API to get the download URL for the dataset. We then "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc, sys\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import OOI M2M tools\n",
    "sys.path.append(\"/home/andrew/Documents/OOI-CGSN/ooinet/ooinet/\")\n",
    "from m2m import M2M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set OOINet API access\n",
    "In order access and download data from OOINet, need to have an OOINet api username and access token. Those can be found on your profile after logging in to OOINet. Your username and access token should NOT be stored in this notebook/python script (for security). It should be stored in a yaml file, kept in the same directory, named user_info.yaml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import user info for connecting to OOINet via M2M\n",
    "userinfo = yaml.load(open(\"../../../../QAQC_Sandbox/user_info.yaml\"), Loader=yaml.FullLoader)\n",
    "username = userinfo[\"apiname\"]\n",
    "token = userinfo[\"apikey\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect to OOINet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OOINet = M2M(username, token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Datasets\n",
    "\n",
    "Identify all of the OOI-CGSN datasets with the **```PCO2W```**, **```PCO2A```**, and **```PHSEN```** datasets that are located at the Global Irminger Array. This is done by querying OOINet and iteratively walking through all of the API endpoints. The results are saved into a csv file so this step doesn't have to be repeated each time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to see if the reference designators have already been identified. If they haven't been previously downloaded, can use the ```OOINet.search_datasets``` function to search for the datasets associated with each instrument.\n",
    "\n",
    "#### PCO2W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # If the reference designators where previously identified and downloaded\n",
    "    pco2w_datasets = pd.read_csv(\"../data/pco2w_datasets.csv\")\n",
    "except:\n",
    "    # Search for PCO2W datasets, asking for English names\n",
    "    pco2w_datasets = OOINet.search_datasets(instrument=\"PCO2W\", English_names=True)\n",
    "\n",
    "    # Save the datasets locally to speed up future runs\n",
    "    pco2w_datasets.to_csv(\"../data/pco2w_datasets.csv\", index=False)\n",
    "\n",
    "# Print out the head\n",
    "pco2w_datasets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PHSEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # If the reference designators where previously identified and downloaded\n",
    "    phsen_datasets = pd.read_csv(\"../data/phsen_datasets.csv\")\n",
    "except:\n",
    "    # Search for PHSEN datasets, asking for full English names\n",
    "    phsen_datasets = OOINet.search_datasets(instrument=\"PHSEN\", English_names=True)\n",
    "\n",
    "    # Save the datasets locally to speed up future runs\n",
    "    phsen_datasets.to_csv(\"../data/phsen_datasets.csv\", index=False)\n",
    "    \n",
    "phsen_datasets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCO2A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # If the reference designators where previously identified and downloaded\n",
    "    pco2a_datasets = pd.read_csv(\"../data/pco2a_datasets.csv\")\n",
    "except:\n",
    "    # Search for PCO2A datasets\n",
    "    pco2a_datasets = OOINet.search_datasets(instrument=\"PCO2A\", English_names=True)\n",
    "\n",
    "    # Save the datasets locally to speed up future runs\n",
    "    pco2a_datasets.to_csv(\"../data/pco2a_datasets.csv\", index=False)\n",
    "    \n",
    "pco2a_datasets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the datasets for the Irminger Array datasets, which start with the prefix \"GI\" for Global Irminger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCO2A\n",
    "mask = pco2a_datasets[\"array\"].apply(lambda x: True if x.startswith(\"GI\") else False)\n",
    "pco2a_datasets = pco2a_datasets[mask]\n",
    "\n",
    "# PCO2W\n",
    "mask = pco2w_datasets[\"array\"].apply(lambda x: True if x.startswith(\"GI\") else False)\n",
    "pco2w_datasets = pco2w_datasets[mask]\n",
    "\n",
    "# PHSEN\n",
    "mask = phsen_datasets[\"array\"].apply(lambda x: True if x.startswith(\"GI\") else False)\n",
    "phsen_datasets = phsen_datasets[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CTD & METBK\n",
    "We will also need the temperature, salinity, and pressure data associated with the carbon system datasets. So we will also identify all the **```CTD```** datasets located at the Global Irminger Array as well as the **```METBK```** dataset for the surface mooring, which has the surface temperature and salinity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # If the reference designators where previously identified and downloaded\n",
    "    ctd_datasets = pd.read_csv(\"../data/ctd_datasets.csv\")\n",
    "except:\n",
    "    # Search for PCO2W datasets, asking for English names\n",
    "    ctd_datasets = OOINet.search_datasets(instrument=\"CTD\", English_names=True)\n",
    "\n",
    "    # Save the datasets locally to speed up future runs\n",
    "    ctd_datasets.to_csv(\"../data/ctd_datasets.csv\", index=False)\n",
    "\n",
    "# Print out the head\n",
    "ctd_datasets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the datasets for the Irminger Array datasets, which start with the prefix \"GI\" for Global Irminger. For the **```CTD```** datasets, we also need to remove the Mobile Asset and Profiler datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the Global Irminger Array datasets\n",
    "mask = ctd_datasets[\"array\"].apply(lambda x: True if x.startswith(\"GP\") else False)\n",
    "ctd_datasets = ctd_datasets[mask]\n",
    "\n",
    "# Remove datasets which are either Glider, AUV, or Profiler Mooring datasets\n",
    "mask = ctd_datasets[\"refdes\"].apply(lambda x: False if \"MOAS\" in x or \"CTDPF\" in x else True)\n",
    "ctd_datasets = ctd_datasets[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # If the reference designators where previously identified and downloaded\n",
    "    metbk_datasets = pd.read_csv(\"../data/metbk_datasets.csv\")\n",
    "except:\n",
    "    # Search for PCO2W datasets, asking for English names\n",
    "    metbk_datasets = OOINet.search_datasets(instrument=\"METBK\", English_names=True)\n",
    "\n",
    "    # Save the datasets locally to speed up future runs\n",
    "    metbk_datasets.to_csv(\"../data/metbk_datasets.csv\", index=False)\n",
    "\n",
    "# Print out the head\n",
    "metbk_datasets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the datasets for the Global Irminger dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = metbk_datasets[\"array\"].apply(lambda x: True if x.startswith(\"GI\") else False)\n",
    "metbk_datasets = metbk_datasets[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Download Datasets\n",
    "Now, download the PCO2A, PHSEN, and PCO2W datasets along with their associated CTD datasets from OOINet and save locally for ease of access. We can scroll through the available datasets to identify which CTD datasets are \n",
    "\n",
    "### Irminger Array\n",
    "* GI01SUMO: Apex Surface Mooring\n",
    "    * SBD12: Surface Buoy\n",
    "        * PCO2AA: pCO2 Air-Sea (refdes = GI01SUMO-SBD12-04-PCO2AA000)\n",
    "        * METBKA: Bulk Meteorology Instrument Package (refdes = GI01SUMO-SBD12-06-METBKA000)\n",
    "    * RID16: Near-Surface Instrument Frame\n",
    "        * PCO2WB: pCO2 Water (refdes = GI01SUMO-RID16-05-PCO2WB000)\n",
    "        * CTDBPF: CTD (refdes = GI01SUMO-RID16-03-CTDBPF000)\n",
    "    * RII11: Mooring Riser\n",
    "        * PCO2WC: pCO2 Water (40 meters) (refdes = GI01SUMO-RII11-02-PCO2WC051)\n",
    "        * CTDMOQ: CTD (40 meters) (refdes = GI01SUMO-RII11-02-CTDBPP031)\n",
    "        * PCO2WC: pCO2 Water (80 meters) (refdes = GI01SUMO-RII11-02-PCO2WC052)\n",
    "        * CTDBPP: CTD (80 meters) (refdes = GI01SUMO-RII11-02-CTDBPP032)\n",
    "        * PCO2WC: pCO2 Water (130 meters) (refdes = GI01SUMO-RII11-02-PCO2WC053)\n",
    "        * CTDBPP: CTD (130 meters) (refdes = GI01SUMO-RII11-02-CTDBPP033)\n",
    "        * PHSENE: Seawater pH (20 meters) (refdes = GI01SUMO-RII11-02-PHSENE041)\n",
    "        * CTDMOQ: CTD (20 meters) (refdes = GI01SUMO-RII11-02-CTDMOQ011)\n",
    "        * PHSENE: Seawater pH (100 meters) (refdes = GI01SUMO-RII11-02-PHSENE042)\n",
    "        * CTDMOQ: CTD (100 meters) (refdes = GI01SUMO-RII11-02-CTDMOQ013)\n",
    "* GI03FLMA: Flanking Subsurface Mooring A\n",
    "    * RIS01: Mooring Riser\n",
    "        * PHSENF: Seawater pH (refdes = GI03FLMA-RIS01-04-PHSENF000)\n",
    "        * CTDMOG: CTD (30 meters) (refdes = GI03FLMA-RIM01-02-CTDMOG040)\n",
    "* GI03FLMB: Flanking Subsurface Mooring B\n",
    "    * RIS01: Mooring Riser\n",
    "        * PHSENF: Seawater pH (refdes = GI03FLMB-RIS01-04-PHSENF000)\n",
    "        * CTDMOG: CTD (30 meters) (refdes = GI03FLMB-RIM01-02-CTDMOG060)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Surface Buoy: PCO2A (GI01SUMO-SBD12-04-PCO2AA000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refdes = \"GP03FLMA-RIS01-04-PHSENF000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = OOINet.get_metadata(refdes)\n",
    "metadata = metadata.groupby(by=[\"refdes\",\"method\",\"stream\"]).agg(lambda x: pd.unique(x.values.ravel()).tolist())\n",
    "metadata = metadata.reset_index()\n",
    "metadata = metadata.applymap(lambda x: x[0] if len(x) == 1 else x)\n",
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Occasionally, we need to further filter the available data streams from the metadata. For example, the METBK returns both computed flux products in the hourly datastream as well as a datastream with the measured met and sea surface data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastreams = OOINet.get_datastreams(refdes)\n",
    "\n",
    "# For METBK: Drop the hourly data streams\n",
    "#mask = datastreams[\"stream\"].apply(lambda x: True if \"hourly\" not in x else False)\n",
    "#datastreams = datastreams[mask]\n",
    "datastreams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the datasets\n",
    "\n",
    "First, we define a function to try to eliminate \"empty\" datasets from the catalog. This occassionally happens depending on the instrument sampling schema, how it records data, and the requested time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_catalog(catalog, stream):\n",
    "    \"\"\"Clean up the netCDF catalog of unwanted datasets\"\"\"\n",
    "    # Parse the netCDF datasets to only get those with the datastream in its name\n",
    "    datasets = []\n",
    "    for dset in catalog:\n",
    "        check = dset.split(\"/\")[-1]\n",
    "        if stream in check:\n",
    "            datasets.append(dset)\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    # Next, check that the netCDF datasets are not empty by getting the timestamps in the\n",
    "    # datasets and checking if they are \n",
    "    catalog = datasets\n",
    "    datasets = []\n",
    "    for dset in catalog:\n",
    "        # Get the timestamps\n",
    "        timestamps = dset.split(\"_\")[-1].replace(\".nc\",\"\").split(\"-\")\n",
    "        t1, t2 = timestamps\n",
    "        # Check if the timestamps are equal\n",
    "        if t1 == t2:\n",
    "            pass\n",
    "        else:\n",
    "            datasets.append(dset)\n",
    "            \n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we iterate over each of the available data streams we identified with good data from the metadata, request the data, open the data into an xarray dataset, and save the dataset to a local directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OOINet.REFDES = refdes\n",
    "refdes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for ind in metadata.index:\n",
    "    row = metadata.loc[ind]\n",
    "    method, stream = row[\"method\"], row[\"stream\"]\n",
    "    if \"power\" in stream or \"blank\" in stream or \"metadata\" in stream or \"control\" in stream or \"offset\" in stream:\n",
    "        continue\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    # Get the thredds url\n",
    "    thredds_url = OOINet.get_thredds_url(refdes, method, stream, goldCopy=True)\n",
    "    print(thredds_url + \"\\n\")\n",
    "    # Access the catalog\n",
    "    catalog = OOINet.get_thredds_catalog(thredds_url)\n",
    "    # Parse the catalog for relevant netCDF files\n",
    "    catalog = OOINet.parse_catalog(catalog, exclude=[\"gps\", \"blank\"])\n",
    "    catalog = sorted(catalog) \n",
    "    # Clean the catalog\n",
    "    catalog = clean_catalog(catalog, stream)\n",
    "       \n",
    "    # Open the data\n",
    "    data = OOINet.load_netCDF_datasets(catalog, goldCopy=True)\n",
    "    \n",
    "    # Eliminate unneeded timestamps\n",
    "    for var in data.variables:\n",
    "        if \"time\" in var and var != \"time\":\n",
    "            data = data.drop_vars(var)\n",
    "        elif data[var].dtype == \"object\":\n",
    "            data = data.drop_vars(var)\n",
    "        else:\n",
    "            pass\n",
    "            \n",
    "    # Download and add annotations\n",
    "    annotations = OOINet.get_annotations(refdes)\n",
    "    data = OOINet.add_annotation_qc_flag(data, annotations)\n",
    "    \n",
    "    # Check that a suitable save directory exists\n",
    "    saveDir = f\"../data/{refdes}/\"\n",
    "    if not os.path.exists(saveDir):\n",
    "        os.makedirs(saveDir)\n",
    "    \n",
    "    # Save the dataset\n",
    "    filename = f\"{refdes}-{method}-{stream}.nc\"\n",
    "    data.to_netcdf(saveDir+filename, engine=\"h5netcdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Datasets\n",
    "\n",
    "Now, we need to merge the data. First, we iterate through the data variables for each dataset, identify any which are unique to a given dataset, and broadcast them to the other datasets. This step is necessary to allow the datasets to combine. Once each dataset has the same data variables, we utilize ```xr.combine_first``` to combine the datasets. We assume that the instrument record, if available, is the best and most complete dataset and utilize the telemetered and recovered_host datasets to fill in the gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../../OS2022/OS2022/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import process_pco2w, process_phsen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refdes = refdes\n",
    "refdes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveDir = f\"../data/{refdes}/\"\n",
    "for file in os.listdir(saveDir):\n",
    "    if file.endswith(\".nc\"):\n",
    "        if \"recovered_inst\" in file:\n",
    "            inst_data = xr.open_dataset(saveDir + file, chunks=\"auto\")\n",
    "            if \"PCO2W\" in refdes:\n",
    "                inst_data = process_pco2w.pco2w_instrument(inst_data)\n",
    "                inst_data.to_netcdf(saveDir+filename, engine=\"h5netcdf\")\n",
    "            elif \"PHSEN\" in refdes:\n",
    "                inst_data = process_phsen.phsen_instrument(inst_data)\n",
    "            else:\n",
    "                pass\n",
    "        elif \"recovered_host\" in file:\n",
    "            host_data = xr.open_dataset(saveDir + file, chunks=\"auto\")\n",
    "            if \"PCO2W\" in refdes:\n",
    "                host_data = process_pco2w.pco2w_datalogger(host_data)\n",
    "            elif \"PHSEN\" in refdes:\n",
    "                host_data = process_phsen.phsen_datalogger(host_data)\n",
    "            else:\n",
    "                pass\n",
    "        elif \"telemetered\" in file:\n",
    "            tele_data = xr.open_dataset(saveDir + file, chunks=\"auto\")\n",
    "            if \"PCO2W\" in refdes:\n",
    "                tele_data = process_pco2w.pco2w_datalogger(tele_data)\n",
    "            elif \"PHSEN\" in refdes:\n",
    "                tele_data = process_phsen.phsen_datalogger(tele_data)\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            pass\n",
    "        # Save the data\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open each method of data delivery and eliminate possible duplicate entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tele_data = tele_data.sel(time=~tele_data.get_index(\"time\").duplicated())\n",
    "host_data = host_data.sel(time=~host_data.get_index(\"time\").duplicated())\n",
    "inst_data = inst_data.sel(time=~inst_data.get_index(\"time\").duplicated())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, need to make sure each dataset has the same variables in order to be able to combine. Can acheive this by iterating over the data variables for each method and utilizing ```xr.broadcast_like``` to create empty arrays for the datasets which are missing the variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to make sure each dataset has the same variables\n",
    "for var in tele_data.variables:\n",
    "    if var not in host_data.variables:\n",
    "        host_data[var] = tele_data[var].broadcast_like(host_data[\"time\"])\n",
    "        \n",
    "for var in host_data.variables:\n",
    "    if var not in tele_data.variables:\n",
    "        tele_data[var] = host_data[var].broadcast_like(tele_data[\"time\"])\n",
    "        \n",
    "tele_host = host_data\n",
    "for var in tele_host.variables:\n",
    "    tele_host[var] = tele_host[var].fillna(value=tele_data[var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the telemetered dataset and host_dataset\n",
    "#tele_host = tele_data.combine_first(host_data)\n",
    "#tele_host = host_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in tele_host.variables:\n",
    "    if var not in inst_data.variables:\n",
    "        inst_data[var] = tele_host[var].broadcast_like(inst_data[\"time\"])\n",
    "\n",
    "for var in inst_data.variables:\n",
    "    if var not in tele_host.variables:\n",
    "        tele_host[var] = inst_data[var].broadcast_like(tele_host[\"time\"])\n",
    "        \n",
    "data = inst_data\n",
    "for var in data:\n",
    "    data[var] = data[var].fillna(value=tele_host[var])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the combined dataset as a netCDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_netcdf(f\"../data/{refdes}_combined.nc\", engine=\"h5netcdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
